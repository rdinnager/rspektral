% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_pool.R
\name{layer_min_cut_pool}
\alias{layer_min_cut_pool}
\title{MinCutPool}
\usage{
layer_min_cut_pool(
  object,
  k,
  mlp_hidden = NULL,
  mlp_activation = "relu",
  return_mask = FALSE,
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  ...
)
}
\description{
\loadmathjax
A minCUT pooling layer as presented by
\href{https://arxiv.org/abs/1907.00481}{Bianchi et al. (2019)}.

\strong{Mode}: batch.

This layer computes a soft clustering \mjeqn{\boldsymbol{S}}{} of the input graphs using a MLP,
and reduces graphs as follows:

\mjdeqn{\boldsymbol{S} = \textrm{MLP}(\boldsymbol{X}); \\\boldsymbol{A}' = \boldsymbol{S}^\top \boldsymbol{A} \boldsymbol{S}; \boldsymbol{X}' = \boldsymbol{S}^\top \boldsymbol{X};}{}

where MLP is a multi-layer perceptron with softmax output.
Two auxiliary loss terms are also added to the model: the \emph{minCUT loss}
\mjdeqn{- \frac{ \mathrm{Tr}(\boldsymbol{S}^\top \boldsymbol{A} \boldsymbol{S}) }{ \mathrm{Tr}(\boldsymbol{S}^\top \boldsymbol{D} \boldsymbol{S}) }}{}
and the \emph{orthogonality loss}
\mjdeqn{\left\|\frac{\boldsymbol{S}^\top \boldsymbol{S}}{\| \boldsymbol{S}^\top \boldsymbol{S} \| _ F}- \frac{\boldsymbol{I} _ K}{\sqrt{K}}\right\| _ F.}{}

The layer can be used without a supervised loss, to compute node clustering
simply by minimizing the two auxiliary losses.

\strong{Input}
\itemize{
\item Node features of shape \verb{([batch], N, F)};
\item Binary adjacency matrix of shape \verb{([batch], N, N)};
}

\strong{Output}
\itemize{
\item Reduced node features of shape \verb{([batch], K, F)};
\item Reduced adjacency matrix of shape \verb{([batch], K, K)};
\item If \code{return_mask=True}, the soft clustering matrix of shape \verb{([batch], N, K)}.
}
}
