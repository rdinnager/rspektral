% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_pool.R
\name{layer_top_k_pool}
\alias{layer_top_k_pool}
\title{TopKPool}
\usage{
layer_top_k_pool(
  object,
  ratio,
  return_mask = FALSE,
  sigmoid_gating = FALSE,
  kernel_initializer = "glorot_uniform",
  kernel_regularizer = NULL,
  kernel_constraint = NULL,
  ...
)
}
\arguments{
\item{ratio}{float between 0 and 1, ratio of nodes to keep in each graph}

\item{return_mask}{boolean, whether to return the binary mask used for pooling}

\item{sigmoid_gating}{boolean, use a sigmoid gating activation instead of a
tanh}

\item{kernel_initializer}{initializer for the weights}

\item{kernel_regularizer}{regularization applied to the weights}

\item{kernel_constraint}{constraint applied to the weights}
}
\description{
\loadmathjax
A gPool/Top-K layer as presented by
\href{http://proceedings.mlr.press/v97/gao19a/gao19a.pdf}{Gao & Ji (2019)} and
\href{https://arxiv.org/abs/1811.01287}{Cangea et al. (2018)}.

\strong{Mode}: single, disjoint.

This layer computes the following operations:

\mjdeqn{\boldsymbol{y} = \frac{\boldsymbol{X}\p}{\|\boldsymbol{p}\|}; \;\;\;\;\boldsymbol{i} = \textrm{rank}(\boldsymbol{y}, K); \;\;\;\;\boldsymbol{X}' = (\boldsymbol{X} \odot \textrm{tanh}(\boldsymbol{y}))_\boldsymbol{i}; \;\;\;\;\boldsymbol{A}' = \boldsymbol{A} _ {\boldsymbol{i}, \boldsymbol{i}}}{}

where \mjeqn{ \textrm{rank}(\boldsymbol{y}, K) }{} returns the indices of the top K values of
\mjeqn{\boldsymbol{y}}{}, and \mjeqn{\boldsymbol{p}}{} is a learnable parameter vector of size \mjeqn{F}{}. \mjeqn{K}{} is
defined for each graph as a fraction of the number of nodes.
Note that the the gating operation \mjeqn{\textrm{tanh}(\boldsymbol{y})}{} (Cangea et al.)
can be replaced with a sigmoid (Gao & Ji).

This layer temporarily makes the adjacency matrix dense in order to compute
\mjeqn{\boldsymbol{A}'}{}.
If memory is not an issue, considerable speedups can be achieved by using
dense graphs directly.
Converting a graph from sparse to dense and back to sparse is an expensive
operation.

\strong{Input}
\itemize{
\item Node features of shape \verb{(N, F)};
\item Binary adjacency matrix of shape \verb{(N, N)};
\item Graph IDs of shape \verb{(N, )} (only in disjoint mode);
}

\strong{Output}
\itemize{
\item Reduced node features of shape \verb{(ratio * N, F)};
\item Reduced adjacency matrix of shape \verb{(ratio * N, ratio * N)};
\item Reduced graph IDs of shape \verb{(ratio * N, )} (only in disjoint mode);
\item If \code{return_mask=True}, the binary pooling mask of shape \verb{(ratio * N, )}.
}
}
