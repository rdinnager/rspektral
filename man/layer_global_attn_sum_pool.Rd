% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_pool.R
\name{layer_global_attn_sum_pool}
\alias{layer_global_attn_sum_pool}
\title{GlobalAttnSumPool}
\usage{
layer_global_attn_sum_pool(
  object,
  attn_kernel_initializer = "glorot_uniform",
  attn_kernel_regularizer = NULL,
  attn_kernel_constraint = NULL,
  ...
)
}
\arguments{
\item{attn_kernel_initializer}{initializer for the attention weights}

\item{attn_kernel_regularizer}{regularization applied to the attention kernel
matrix}

\item{attn_kernel_constraint}{constraint applied to the attention kernel
matrix}
}
\description{
\loadmathjax
A node-attention global pooling layer. Pools a graph by learning attention
coefficients to sum node features.

This layer computes:
\mjdeqn{\alpha = \textrm{softmax}( \boldsymbol{X} \boldsymbol{a}); \\\boldsymbol{X}' = \sum\limits_{i=1}^{N} \alpha_i \cdot \boldsymbol{X} _ i}{}
where \mjeqn{\boldsymbol{a} \in \mathbb{R}^F}{} is a trainable vector. Note that the softmax
is applied across nodes, and not across features.

\strong{Mode}: single, disjoint, mixed, batch.

\strong{Input}
\itemize{
\item Node features of shape \verb{([batch], N, F)};
\item Graph IDs of shape \verb{(N, )} (only in disjoint mode);
}

\strong{Output}
\itemize{
\item Pooled node features of shape \verb{(batch, F)} (if single mode, shape will
be \verb{(1, F)}).
}
}
