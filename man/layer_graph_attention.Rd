% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_conv.R
\name{layer_graph_attention}
\alias{layer_graph_attention}
\title{GraphAttention}
\usage{
layer_graph_attention(
  object,
  channels,
  attn_heads = 1,
  concat_heads = TRUE,
  dropout_rate = 0.5,
  return_attn_coef = FALSE,
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  attn_kernel_initializer = "glorot_uniform",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  attn_kernel_regularizer = NULL,
  activity_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  attn_kernel_constraint = NULL,
  ...
)
}
\arguments{
\item{channels}{number of output channels}

\item{attn_heads}{number of attention heads to use}

\item{concat_heads}{bool, whether to concatenate the output of the attention
heads instead of averaging}

\item{dropout_rate}{internal dropout rate for attention coefficients}

\item{return_attn_coef}{if True, return the attention coefficients for
the given input (one N x N matrix for each head).}

\item{activation}{activation function to use}

\item{use_bias}{bool, add a bias vector to the output}

\item{kernel_initializer}{initializer for the weights}

\item{bias_initializer}{initializer for the bias vector}

\item{attn_kernel_initializer}{initializer for the attention weights}

\item{kernel_regularizer}{regularization applied to the weights}

\item{bias_regularizer}{regularization applied to the bias vector}

\item{attn_kernel_regularizer}{regularization applied to the attention kernels}

\item{activity_regularizer}{regularization applied to the output}

\item{kernel_constraint}{constraint applied to the weights}

\item{bias_constraint}{constraint applied to the bias vector.}

\item{attn_kernel_constraint}{constraint applied to the attention kernels}
}
\description{
\loadmathjax
A graph attention layer (GAT) as presented by
\href{https://arxiv.org/abs/1710.10903}{Velickovic et al. (2017)}.

\strong{Mode}: single, disjoint, mixed, batch.

\strong{This layer expects dense inputs when working in batch mode.}

This layer computes a convolution similar to \code{layers.GraphConv}, but
uses the attention mechanism to weight the adjacency matrix instead of
using the normalized Laplacian:
\mjdeqn{ Z = \mathbf{\alpha}XW + b }{}
where
\mjdeqn{ \mathbf{\alpha} _ {ij} = \frac{ \exp\left( \mathrm{LeakyReLU}\left( a^{\top} \link[=(XW)_i \\, \\| \\, (XW)_j]{(XW)_i \, \| \, (XW)_j} \right) \right) } {\sum\limits_{k \in \mathcal{N}(i) \cup \{ i \}} \exp\left( \mathrm{LeakyReLU}\left( a^{\top} \link[=(XW)_i \\, \\| \\, (XW)_k]{(XW)_i \, \| \, (XW)_k} \right) \right) } }{}
where \mjeqn{a \in \mathbb{R}^{2F'}}{} is a trainable attention kernel.
Dropout is also applied to \mjeqn{\alpha}{} before computing \mjeqn{Z}{}.
Parallel attention heads are computed in parallel and their results are
aggregated by concatenation or average.

\strong{Input}
\itemize{
\item Node features of shape \verb{([batch], N, F)};
\item Binary adjacency matrix of shape \verb{([batch], N, N)};
}

\strong{Output}
\itemize{
\item Node features with the same shape as the input, but with the last
dimension changed to \code{channels};
\item if \code{return_attn_coef=True}, a list with the attention coefficients for
each attention head. Each attention coefficient matrix has shape
\verb{([batch], N, N)}.
}
}
