% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_cheb_conv}
\alias{layer_cheb_conv}
\title{A Chebyshev graph convolutional layer.
A Chebyshev convolutional layer as presented by
\href{https://arxiv.org/abs/1606.09375}{Defferrard et al. (2016)}. \cr
\strong{Mode}: single, disjoint, mixed, batch. This layer computes:
$$ \Z = \sum \limits_{k=0}^{K - 1} \T^{(k)} \W^{(k)} + \b^{(k)},
$$
where \( \T^{(0)}, ..., \T^{(K - 1)} \) are Chebyshev polynomials of \(\tilde \L\)
defined as
$$ \T^{(0)} = \X \\ \T^{(1)} = \tilde \L \X \\ \T^{(k \ge 2)} = 2 \cdot \tilde \L \T^{(k - 1)} - \T^{(k - 2)},
$$
where
$$ \tilde \L = \frac{2}{\lambda_{max}} \cdot (\I - \D^{-1/2} \A \D^{-1/2}) - \I
$$
is the normalized Laplacian with a rescaled spectrum. \strong{Input} - Node features of shape \verb{([batch], N, F)};
\itemize{
\item A list of K Chebyshev polynomials of shape
\verb{[([batch], N, N), ..., ([batch], N, N)]}; can be computed with
\code{spektral.utils.convolution.chebyshev_filter}. \strong{Output} - Node features with the same shape of the input, but with the last
dimension changed to \code{channels}.
}}
\usage{
layer_cheb_conv(
  object,
  channels,
  K = 1L,
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  activity_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  ...
)
}
\arguments{
\item{object}{model or layer object;}

\item{channels}{number of output channels;}

\item{K}{order of the Chebyshev polynomials;}

\item{activation}{activation function to use;}

\item{use_bias}{bool, add a bias vector to the output;}

\item{kernel_initializer}{initializer for the weights;}

\item{bias_initializer}{initializer for the bias vector;}

\item{kernel_regularizer}{regularization applied to the weights;}

\item{bias_regularizer}{regularization applied to the bias vector;}

\item{activity_regularizer}{regularization applied to the output;}

\item{kernel_constraint}{constraint applied to the weights;}

\item{bias_constraint}{constraint applied to the bias vector.}
}
\description{
A Chebyshev graph convolutional layer.
A Chebyshev convolutional layer as presented by
\href{https://arxiv.org/abs/1606.09375}{Defferrard et al. (2016)}. \cr
\strong{Mode}: single, disjoint, mixed, batch. This layer computes:
$$ \Z = \sum \limits_{k=0}^{K - 1} \T^{(k)} \W^{(k)} + \b^{(k)},
$$
where \( \T^{(0)}, ..., \T^{(K - 1)} \) are Chebyshev polynomials of \(\tilde \L\)
defined as
$$ \T^{(0)} = \X \\ \T^{(1)} = \tilde \L \X \\ \T^{(k \ge 2)} = 2 \cdot \tilde \L \T^{(k - 1)} - \T^{(k - 2)},
$$
where
$$ \tilde \L = \frac{2}{\lambda_{max}} \cdot (\I - \D^{-1/2} \A \D^{-1/2}) - \I
$$
is the normalized Laplacian with a rescaled spectrum. \strong{Input} - Node features of shape \verb{([batch], N, F)};
\itemize{
\item A list of K Chebyshev polynomials of shape
\verb{[([batch], N, N), ..., ([batch], N, N)]}; can be computed with
\code{spektral.utils.convolution.chebyshev_filter}. \strong{Output} - Node features with the same shape of the input, but with the last
dimension changed to \code{channels}.
}
}
