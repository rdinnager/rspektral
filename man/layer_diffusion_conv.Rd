% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_conv.R
\name{layer_diffusion_conv}
\alias{layer_diffusion_conv}
\title{DiffusionConv}
\usage{
layer_diffusion_conv(
  object,
  channels,
  num_diffusion_steps = 6,
  kernel_initializer = "glorot_uniform",
  kernel_regularizer = NULL,
  kernel_constraint = NULL,
  activation = "tanh",
  ...
)
}
\arguments{
\item{channels}{number of output channels}

\item{num_diffusion_steps}{How many diffusion steps to consider. \(K\) in paper.}

\item{kernel_initializer}{initializer for the weights}

\item{kernel_regularizer}{regularization applied to the weights}

\item{kernel_constraint}{constraint applied to the weights}

\item{activation}{activation function \(\sigma\) (\(\tanh\) by default)}
}
\description{
\loadmathjax Applies Graph Diffusion Convolution as descibed by
\href{https://arxiv.org/pdf/1707.01926.pdf}{Li et al. (2016)}

\strong{Mode}: single, disjoint, mixed, batch.

\strong{This layer expects a dense adjacency matrix.}

Given a number of diffusion steps \mjeqn{K}{} and a row normalized adjacency matrix \mjeqn{\hat A }{},
this layer calculates the q'th channel as:
\mjdeqn{ \mathbf{H} _ {~:,~q} = \sigma\left( \sum_{f=1}^{F} \left( \sum_{k=0}^{K-1}\theta_k {\hat A}^k \right) X_{~:,~f} \right) }{}

\strong{Input}
\itemize{
\item Node features of shape \verb{([batch], N, F)};
\item Normalized adjacency or attention coef. matrix \mjeqn{\hat A }{} of shape
\verb{([batch], N, N)}; Use \code{DiffusionConvolution.preprocess} to normalize.
}

\strong{Output}
\itemize{
\item Node features with the same shape as the input, but with the last
dimension changed to \code{channels}.
}
}
