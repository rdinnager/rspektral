% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_pool.R
\name{layer_global_attention_pool}
\alias{layer_global_attention_pool}
\title{GlobalAttentionPool}
\usage{
layer_global_attention_pool(
  object,
  channels,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  ...
)
}
\arguments{
\item{channels}{integer, number of output channels}

\item{kernel_initializer}{NA}

\item{bias_initializer}{initializer for the bias vectors}

\item{kernel_regularizer}{regularization applied to the kernel matrices}

\item{bias_regularizer}{regularization applied to the bias vectors}

\item{kernel_constraint}{constraint applied to the kernel matrices}

\item{bias_constraint}{constraint applied to the bias vectors.}
}
\description{
\loadmathjax
A gated attention global pooling layer as presented by
\href{https://arxiv.org/abs/1511.05493}{Li et al. (2017)}.

This layer computes:
\mjdeqn{\boldsymbol{X}' = \sum\limits_{i=1}^{N} (\sigma(\boldsymbol{X} \boldsymbol{W} _ 1 + \boldsymbol{b} _ 1) \odot (\boldsymbol{X} \boldsymbol{W} _ 2 + \boldsymbol{b} _ 2))_i}{}
where \mjeqn{\sigma}{} is the sigmoid activation function.

\strong{Mode}: single, disjoint, mixed, batch.

\strong{Input}
\itemize{
\item Node features of shape \verb{([batch], N, F)};
\item Graph IDs of shape \verb{(N, )} (only in disjoint mode);
}

\strong{Output}
\itemize{
\item Pooled node features of shape \verb{(batch, channels)} (if single mode,
shape will be \verb{(1, channels)}).
}
}
