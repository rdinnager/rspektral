% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers_conv.R
\name{layer_gin_conv}
\alias{layer_gin_conv}
\title{GINConv}
\usage{
layer_gin_conv(
  object,
  channels,
  epsilon = NULL,
  mlp_hidden = NULL,
  mlp_activation = "relu",
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  bias_initializer = "zeros",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  activity_regularizer = NULL,
  kernel_constraint = NULL,
  bias_constraint = NULL,
  ...
)
}
\arguments{
\item{channels}{integer, number of output channels}

\item{epsilon}{unnamed parameter, see
\href{https://arxiv.org/abs/1810.00826}{Xu et al. (2018)}, and the equation above.
By setting \code{epsilon=None}, the parameter will be learned (default behaviour).
If given as a value, the parameter will stay fixed.}

\item{mlp_hidden}{list of integers, number of hidden units for each hidden
layer in the MLP (if None, the MLP has only the output layer)}

\item{mlp_activation}{activation for the MLP layers}

\item{activation}{activation function to use}

\item{use_bias}{bool, add a bias vector to the output}

\item{kernel_initializer}{initializer for the weights}

\item{bias_initializer}{initializer for the bias vector}

\item{kernel_regularizer}{regularization applied to the weights}

\item{bias_regularizer}{regularization applied to the bias vector}

\item{activity_regularizer}{regularization applied to the output}

\item{kernel_constraint}{constraint applied to the weights}

\item{bias_constraint}{constraint applied to the bias vector.}
}
\description{
\loadmathjax
A Graph Isomorphism Network (GIN) as presented by
\href{https://arxiv.org/abs/1810.00826}{Xu et al. (2018)}.

\strong{Mode}: single, disjoint.

\strong{This layer expects a sparse adjacency matrix.}

This layer computes for each node \mjeqn{i}{}:
\mjdeqn{ Z_i = \textrm{MLP}\big( (1 + \epsilon) \cdot X_i + \sum\limits_{j \in \mathcal{N}(i)} X_j \big) }{}
where \mjeqn{\textrm{MLP}}{} is a multi-layer perceptron.

\strong{Input}
\itemize{
\item Node features of shape \verb{(N, F)};
\item Binary adjacency matrix of shape \verb{(N, N)}.
}

\strong{Output}
\itemize{
\item Node features with the same shape of the input, but the last dimension
changed to \code{channels}.
}
}
